{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c26220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import IPython\n",
    "import IPython.display\n",
    "from ipywidgets import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "import scipy\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "\n",
    "from andi_datasets.models_phenom import models_phenom\n",
    "from andi_datasets.datasets_phenom import datasets_phenom\n",
    "from andi_datasets.utils_trajectories import plot_trajs\n",
    "from andi_datasets.utils_challenge import label_continuous_to_list\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DENSITY_NB = 25\n",
    "EXT_WIDTH = 100\n",
    "WIN_WIDTHS = np.arange(20, 100, 2)\n",
    "\n",
    "colormap = 'jet'  # matplotlib colormap\n",
    "mycmap = plt.get_cmap(colormap, lut=None)\n",
    "color_seq = [mycmap(i)[:3] for i in range(mycmap.N)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f266a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncumulate(xs:np.ndarray):\n",
    "    assert xs.ndim == 1\n",
    "    uncum_list = [0.]\n",
    "    for i in range(1, len(xs)):\n",
    "        uncum_list.append(xs[i] - xs[i-1])\n",
    "    return np.array(uncum_list).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8138640-913d-40ba-8c1c-dcc0c9f3e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radius_list(xs:np.ndarray, ys:np.ndarray):\n",
    "    assert xs.ndim == 1 and ys.ndim == 1\n",
    "    rad_list = [0.]\n",
    "    disp_list = []\n",
    "    for i in range(1, len(xs)):\n",
    "        rad_list.append(np.sqrt((xs[i] - xs[0])**2 + (ys[i] - ys[0])**2))\n",
    "        disp_list.append(np.sqrt((xs[i] - xs[i-1])**2 + (ys[i] - ys[i-1])**2))\n",
    "    return np.array(rad_list) / np.mean(disp_list) / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signal(x_pos, y_pos, win_widths):\n",
    "    all_vals = []\n",
    "    for win_width in win_widths:\n",
    "        if win_width >= len(x_pos):\n",
    "            continue\n",
    "        vals = []\n",
    "        for checkpoint in range(win_width//2, len(x_pos) - win_width//2):\n",
    "            xs = x_pos[checkpoint - int(win_width/2) : checkpoint + int(win_width/2)]\n",
    "            ys = y_pos[checkpoint - int(win_width/2) : checkpoint + int(win_width/2)]\n",
    "\n",
    "            xs1 = xs[1: int(len(xs)/2)+1] - float(xs[1: int(len(xs)/2)+1][0])\n",
    "            xs2 = xs[int(len(xs)/2):] - float(xs[int(len(xs)/2):][0])\n",
    "            ys1 = ys[1: int(len(ys)/2)+1] - float(ys[1: int(len(ys)/2)+1][0])\n",
    "            ys2 = ys[int(len(ys)/2):] - float(ys[int(len(ys)/2):][0])        \n",
    "\n",
    "            cum_xs1 = abs(np.cumsum(abs(xs1)))\n",
    "            cum_xs2 = abs(np.cumsum(abs(xs2)))\n",
    "            cum_ys1 = abs(np.cumsum(abs(ys1)))\n",
    "            cum_ys2 = abs(np.cumsum(abs(ys2)))\n",
    "\n",
    "            xs_max_val = max(np.max(abs(cum_xs1)), np.max(abs(cum_xs2)))\n",
    "            cum_xs1 = cum_xs1 / xs_max_val\n",
    "            cum_xs2 = cum_xs2 / xs_max_val\n",
    "\n",
    "            ys_max_val = max(np.max(abs(cum_ys1)), np.max(abs(cum_ys2)))\n",
    "            cum_ys1 = cum_ys1 / ys_max_val\n",
    "            cum_ys2 = cum_ys2 / ys_max_val\n",
    "            \n",
    "            vals.append((abs(cum_xs1[-1] - cum_xs2[-1] + cum_ys1[-1] - cum_ys2[-1]))\n",
    "                        + (max(np.std(xs1), np.std(xs2)) - min(np.std(xs1), np.std(xs2)))\n",
    "                        + (max(np.std(ys1), np.std(ys2)) - min(np.std(ys1), np.std(ys2))))\n",
    "                    \n",
    "        vals = np.concatenate((np.ones(int(win_width/2)) * 0, vals))\n",
    "        vals = np.concatenate((vals, np.ones(int(win_width/2)) * 0))\n",
    "        vals = np.array(vals)\n",
    "        all_vals.append(vals)\n",
    "\n",
    "    all_vals = np.array(all_vals) + 1e-5\n",
    "    normalized_vals = all_vals.copy()\n",
    "    for i in range(len(normalized_vals)):\n",
    "            normalized_vals[i] = normalized_vals[i] / np.max(normalized_vals[i])\n",
    "    return all_vals, normalized_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(signal_seq, jump_d, ext_width, shift_width):\n",
    "    slice_d = []\n",
    "    indice = []\n",
    "    for i in range(ext_width, signal_seq.shape[1] - ext_width, jump_d):\n",
    "        crop = signal_seq[:, i - shift_width//2: i + shift_width//2]\n",
    "        slice_d.append(crop)\n",
    "        indice.append(i)\n",
    "    return np.array(slice_d), np.array(indice) - ext_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climb_mountain(signal, cp, seuil=5):\n",
    "    while True:\n",
    "        vals = [signal[x] if 0<=x<signal.shape[0] else -1 for x in range(cp-seuil,cp+1+seuil)]\n",
    "        if len(vals) == 0:\n",
    "            return -1\n",
    "        new_cp = cp + np.argmax(vals) - seuil\n",
    "        if new_cp == cp:\n",
    "            return new_cp\n",
    "        else:\n",
    "            cp = new_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf24fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_extension(x, y, ext_width):\n",
    "    datas = []\n",
    "    for data in [x, y]:\n",
    "        delta_prev_data = -uncumulate(data[:min(data.shape[0], ext_width)])[1:]\n",
    "        delta_prev_data[0] += float(data[0])\n",
    "        prev_data = np.cumsum(delta_prev_data)[::-1]\n",
    "\n",
    "        delta_next_data = -uncumulate(data[data.shape[0] - min(data.shape[0], ext_width):][::-1])[1:]\n",
    "        delta_next_data[0] += float(data[-1])\n",
    "        next_data = np.cumsum(delta_next_data)\n",
    "\n",
    "        ext_data = np.concatenate((prev_data, data))\n",
    "        ext_data = np.concatenate((ext_data, next_data))\n",
    "        datas.append(ext_data)\n",
    "    return np.array(datas), delta_prev_data.shape[0], delta_next_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e47ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, beta=3):\n",
    "    x = np.minimum(np.ones_like(x)*0.999, x)\n",
    "    x = np.maximum(np.ones_like(x)*0.001, x)\n",
    "    return 1 / (1 + (x / (1-x))**(-beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dbf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimation(x, y, ext1, ext2, max_nb):\n",
    "    densities = []\n",
    "    dist_amp = 2.0\n",
    "    local_mean_window_size = 5\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        density1 = 0\n",
    "        density2 = 0\n",
    "        \n",
    "        slice_x = x[max(0, i-max_nb//2):i].copy()\n",
    "        slice_y = y[max(0, i-max_nb//2):i].copy()\n",
    "        \n",
    "        if len(slice_x) > 0:\n",
    "            mean_dist = np.sqrt(uncumulate(slice_x)**2 + uncumulate(slice_y)**2).mean()\n",
    "            mean_dist *= dist_amp\n",
    "\n",
    "            slice_x -= slice_x[len(slice_x)//2]\n",
    "            slice_y -= slice_y[len(slice_y)//2]\n",
    "            for s_x, s_y in zip(slice_x, slice_y):\n",
    "                if np.sqrt(s_x**2 + s_y**2) < mean_dist:\n",
    "                    density1 += 1\n",
    "    \n",
    "        slice_x = x[i:min(x.shape[0], i+max_nb//2)].copy()\n",
    "        slice_y = y[i:min(x.shape[0], i+max_nb//2)].copy()\n",
    "                \n",
    "        if len(slice_x) > 0:\n",
    "            mean_dist = np.sqrt(uncumulate(slice_x)**2 + uncumulate(slice_y)**2).mean()\n",
    "            mean_dist *= dist_amp\n",
    "\n",
    "            slice_x -= slice_x[len(slice_x)//2]\n",
    "            slice_y -= slice_y[len(slice_y)//2]\n",
    "            for s_x, s_y in zip(slice_x, slice_y):\n",
    "                if np.sqrt(s_x**2 + s_y**2) < mean_dist:\n",
    "                    density2 += 1\n",
    "        densities.append(max(density1, density2))\n",
    "        \n",
    "    \n",
    "    # local_mean\n",
    "    new_densities = []\n",
    "    for i in range(len(densities)):\n",
    "        new_densities.append(np.mean(densities[max(0, i-local_mean_window_size//2): \n",
    "                                               min(len(densities), i+local_mean_window_size//2+1)]))\n",
    "    densities = new_densities\n",
    "        \n",
    "    return np.array(densities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_from_extended_data(x, y, win_widths, ext_width, jump_d, shift_width):\n",
    "    assert ext_width > shift_width\n",
    "    datas, shape_ext1, shape_ext2 = position_extension(x, y, ext_width)\n",
    "    signal, norm_signal = make_signal(datas[0], datas[1], win_widths)\n",
    "\n",
    "    density = density_estimation(datas[0], datas[1], \n",
    "                                 shape_ext1, shape_ext2, \n",
    "                                 max_nb=MAX_DENSITY_NB*2)\n",
    "\n",
    "    denoised_den = denoise_tv_chambolle(density, weight=3, eps=0.0002, max_num_iter=100, channel_axis=None)\n",
    "    denoised_den = sigmoid(denoised_den / MAX_DENSITY_NB)\n",
    "    #denoised_den = 1\n",
    "    \n",
    "    signal = signal[:,] * denoised_den\n",
    "    sliced_signals, slice_indice = slice_data(signal, jump_d, shape_ext1, shift_width)\n",
    "    \n",
    "    return (signal[:, shape_ext1:signal.shape[1] - shape_ext2],\n",
    "            sliced_signals,\n",
    "            slice_indice,\n",
    "            signal,\n",
    "            denoised_den,\n",
    "            shape_ext1, shape_ext2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_roughness(signal, window_size):\n",
    "    uc_signal = uncumulate(signal)\n",
    "    uc_signal /= abs(uc_signal)\n",
    "    counts = []\n",
    "    for i in range(window_size//2, len(uc_signal) - window_size//2):\n",
    "        count = 0\n",
    "        cur_state = 1\n",
    "        for j in range(i-window_size//2, i+window_size//2):\n",
    "            new_state = uc_signal[j]\n",
    "            if new_state != cur_state:\n",
    "                count += 1\n",
    "            cur_state = new_state\n",
    "        counts.append(count)\n",
    "    counts = np.concatenate(([counts[0]] * (window_size//2), counts))\n",
    "    counts = np.concatenate((counts, [counts[-1]] * (window_size//2)))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_normalize(slices):\n",
    "    val = np.mean(np.sum(slices, axis=(2)).T, axis=0)\n",
    "    val = val - np.min(val)\n",
    "    val = val / np.max(val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa89be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749cf97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_data_path = 'public_data_validation_v1/' # make sure the folder has this name or change it\n",
    "track = 2\n",
    "exp = 5\n",
    "fovs = [0]\n",
    "traj_idx = [0] # 0exp0fov of 3, 11, 13, 15, 16, 38, 46 check,, exp10,fov0,idx7 check\n",
    "        \n",
    "for fov in fovs:\n",
    "    # We read the corresponding csv file from the public data and extract the indices of the trajectories:\n",
    "    if track == 2:\n",
    "        df = pd.read_csv(public_data_path + f'track_{track}/exp_{exp}/trajs_fov_{fov}.csv')\n",
    "    else:\n",
    "        df = pd.read_csv(public_data_path + f'track_{track}/exp_{exp}/videos_fov_{fov}_track.csv')\n",
    "\n",
    "    for idx in traj_idx:\n",
    "        # Get the lenght of the trajectory\n",
    "        x = np.array(df[df.traj_idx == idx])[:, 2]\n",
    "        y = np.array(df[df.traj_idx == idx])[:, 3]\n",
    "changepoints = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d71dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_model, labels_model = models_phenom().single_state(N=2,\n",
    "                                                        L=None,\n",
    "                                                        T=500,\n",
    "                                                        alphas=1.5,\n",
    "                                                        Ds=0.1,\n",
    "                                                       )\n",
    "x = trajs_model[:, 0, 0]\n",
    "y = trajs_model[:, 0, 1]\n",
    "changepoints, alphas_cp, Ds, state_num = label_continuous_to_list(labels_model[:, 0, :])\n",
    "changepoints = changepoints[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_model, labels_model = models_phenom().multi_state(N=2,\n",
    "                                                        L=None,\n",
    "                                                        T=500,\n",
    "                                                        alphas=[0.5, 1.5],\n",
    "                                                        Ds=[[0.1, 0.0], [0.1, 0.0]],\n",
    "                                                        M=[[0.99, 0.01], [0.01, 0.99]]\n",
    "                                                       )\n",
    "x = trajs_model[:, 0, 0]\n",
    "y = trajs_model[:, 0, 1]\n",
    "changepoints, alphas_cp, Ds, state_num = label_continuous_to_list(labels_model[:, 0, :])\n",
    "changepoints = changepoints[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd898ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'GT: {changepoints}, Length:{x.shape[0]}')\n",
    "checkpoints = [0]\n",
    "\n",
    "signals, sliced_signals, _, _, denoised_density, l_ext,_ = signal_from_extended_data(x, y,\n",
    "                                                                            WIN_WIDTHS,\n",
    "                                                                            EXT_WIDTH,\n",
    "                                                                            1,\n",
    "                                                                            10)\n",
    "slice_norm_signal = slice_normalize(sliced_signals)\n",
    "print(f'sig_mean: {np.mean(slice_norm_signal)}')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(signals.shape[0]):\n",
    "    plt.plot(np.arange(signals.shape[1]), signals[i] / np.max(signals[i]), alpha=0.1,\n",
    "             c=color_seq[int(i * 255 / signals.shape[0])])\n",
    "plt.plot(np.arange(signals.shape[1]), slice_norm_signal, c='red')\n",
    "for cp in changepoints:\n",
    "    plt.vlines(cp, 0, 1, color='blue')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "if not isinstance(denoised_density, int):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.arange(signals.shape[1]), denoised_density[l_ext:l_ext + signals.shape[1]], c='brown')\n",
    "    for cp in changepoints:\n",
    "        plt.vlines(cp, 0, 1, color='blue')\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x, y, linewidth=0.8)\n",
    "for checkpoint in checkpoints:\n",
    "    plt.scatter(x[checkpoint], y[checkpoint], marker='+', c='green')\n",
    "plt.xlim([np.min(x) - 15,\n",
    "                  np.min(x) + max(np.max(x) - np.min(x), \n",
    "                    np.max(y) - np.min(y)) + 15])\n",
    "plt.ylim([np.min(y) - 15,\n",
    "                  np.min(y) + max(np.max(x) - np.min(x), \n",
    "                    np.max(y) - np.min(y)) + 15])\n",
    "plt.scatter(x[0], y[0], marker='>', c='red')\n",
    "plt.scatter(x[-1], y[-1], marker='<', c='blue')\n",
    "for cp in changepoints:\n",
    "    plt.scatter(x[cp], y[cp], marker='+', c='blue', zorder=2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(x, y, np.arange(signals.shape[1]), linewidth=0.8, alpha=0.7)\n",
    "ax.set_xlim([np.min(x) - 15,\n",
    "                  np.min(x) + max(np.max(x) - np.min(x), \n",
    "                    np.max(y) - np.min(y)) + 15])\n",
    "ax.set_ylim([np.min(y) - 15,\n",
    "                  np.min(y) + max(np.max(x) - np.min(x), \n",
    "                    np.max(y) - np.min(y)) + 15])\n",
    "ax.scatter3D(x[0], y[0], 0, marker='>', c='red')\n",
    "ax.scatter3D(x[-1], y[-1], signals.shape[1], marker='<', c='blue')\n",
    "\n",
    "for cp in changepoints:\n",
    "    ax.scatter3D(x[cp], y[cp], np.arange(signals.shape[1])[cp], marker='+', c='blue', zorder=2)\n",
    "for checkpoint in checkpoints:\n",
    "    ax.scatter3D(x[checkpoint], y[checkpoint], np.arange(signals.shape[1])[checkpoint], marker='+', c='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_comparison(x_pos, y_pos, win_widths, ext_width, checkpoint1, checkpoint2, *args):\n",
    "    all_vals = []\n",
    "    for win_width in win_widths:\n",
    "        if win_width >= len(x_pos):\n",
    "            continue\n",
    "        vals = []\n",
    "        xs1 = x_pos[ext_width + checkpoint1 - int(win_width/2) : ext_width + checkpoint1 + int(win_width/2)]\n",
    "        ys1 = y_pos[ext_width + checkpoint1 - int(win_width/2) : ext_width + checkpoint1 + int(win_width/2)]\n",
    "        \n",
    "        xs2 = x_pos[ext_width + checkpoint2 - int(win_width/2) : ext_width + checkpoint2 + int(win_width/2)]\n",
    "        ys2 = y_pos[ext_width + checkpoint2 - int(win_width/2) : ext_width + checkpoint2 + int(win_width/2)]\n",
    "        \n",
    "        xs1_l = xs1[1: int(len(xs1)/2)+1] - float(xs1[1: int(len(xs1)/2)+1][0])\n",
    "        xs1_r = xs1[int(len(xs1)/2):] - float(xs1[int(len(xs1)/2):][0])\n",
    "\n",
    "        ys1_l = ys1[1: int(len(ys1)/2)+1] - float(ys1[1: int(len(ys1)/2)+1][0])\n",
    "        ys1_r = ys1[int(len(ys1)/2):] - float(ys1[int(len(ys1)/2):][0])\n",
    "        \n",
    "        xs2_l = xs2[1: int(len(xs2)/2)+1] - float(xs2[1: int(len(xs2)/2)+1][0])\n",
    "        xs2_r = xs2[int(len(xs2)/2):] - float(xs2[int(len(xs2)/2):][0])\n",
    "\n",
    "        ys2_l = ys2[1: int(len(ys2)/2)+1] - float(ys2[1: int(len(ys2)/2)+1][0])\n",
    "        ys2_r = ys2[int(len(ys2)/2):] - float(ys2[int(len(ys2)/2):][0])\n",
    "        \n",
    "        std_xs1_l = np.std(xs1_l)\n",
    "        std_xs1_r = np.std(xs1_r)\n",
    "        std_ys1_l = np.std(ys1_l)\n",
    "        std_ys1_r = np.std(ys1_r)\n",
    "        std_xs2_l = np.std(xs2_l)\n",
    "        std_xs2_r = np.std(xs2_r)\n",
    "        std_ys2_l = np.std(ys2_l)\n",
    "        std_ys2_r = np.std(ys2_r)\n",
    "        \n",
    "        xs1_l = np.cumsum(abs(xs1_l))\n",
    "        xs1_r = np.cumsum(abs(xs1_r))\n",
    "        ys1_l = np.cumsum(abs(ys1_l))\n",
    "        ys1_r = np.cumsum(abs(ys1_r))\n",
    "        xs2_l = np.cumsum(abs(xs2_l))\n",
    "        xs2_r = np.cumsum(abs(xs2_r))\n",
    "        ys2_l = np.cumsum(abs(ys2_l))\n",
    "        ys2_r = np.cumsum(abs(ys2_r))\n",
    "\n",
    "        #xs1_l, xs1_r, ys1_l, ys1_r\n",
    "        #xs2_l, xs2_r, ys2_l, ys2_r\n",
    "        comb = [[[xs1_l, xs2_r], [ys1_l, ys2_r]], [[xs1_r, xs2_r], [ys1_r, ys2_r]],\n",
    "               [[xs1_l, xs2_l], [ys1_l, ys2_l]], [[xs1_r, xs2_l], [ys1_r, ys2_l]]]\n",
    "        comb2 = [[std_xs1_l, std_xs2_r, std_ys1_l, std_ys2_r], [std_xs1_r, std_xs2_r, std_ys1_r, std_ys2_r],\n",
    "                 [std_xs1_l, std_xs2_l, std_ys1_l, std_ys2_l], [std_xs1_r, std_xs2_l, std_ys1_r, std_ys2_l]]\n",
    "                 \n",
    "        for cb, cb2 in zip(comb, comb2):\n",
    "            tmp_vals = []\n",
    "            for s in cb:\n",
    "                l, r = s[0], s[1]\n",
    "                max_val = max(np.max(abs(l)), np.max(abs(r)))\n",
    "                l_val = l / max_val\n",
    "                r_val = r / max_val\n",
    "                tmp_vals.append(l_val)\n",
    "                tmp_vals.append(r_val)\n",
    "            val = abs(tmp_vals[0][-1] - tmp_vals[1][-1] + tmp_vals[2][-1] - tmp_vals[3][-1])\n",
    "                       #+ (max(cb2[0], cb2[1]) - min(cb2[0], cb2[1]))\n",
    "                       #+ (max(cb2[2], cb2[3]) - min(cb2[2], cb2[3])))\n",
    "            vals.append(val)\n",
    "        vals = np.array(vals)\n",
    "        all_vals.append(vals)\n",
    "    all_vals = np.array(all_vals)\n",
    "    mean_vals = np.sort(all_vals.T, axis=1)[:, int(len(win_widths) * 0.25): int(len(win_widths) * 0.75)].mean(axis=1)\n",
    "    \n",
    "    if checkpoint1 in args:\n",
    "        print(checkpoint1, checkpoint2, mean_vals, win_widths[np.argmax(all_vals, axis=0)[0]])\n",
    "        print(all_vals)\n",
    "        \n",
    "    if (mean_vals[1] + mean_vals[2]) / 2 >= 1 and (mean_vals[0] + mean_vals[3]) / 2 < 1 and mean_vals[1] > 1 and mean_vals[2] > 1:\n",
    "        return 1\n",
    "    elif (mean_vals[1] + mean_vals[2]) / 2 < 1. and (mean_vals[0] + mean_vals[3]) / 2 >= 1 and mean_vals[0] > 1 and mean_vals[3] > 1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pts_mat(checkpoints, x, y, win_widths, ext_width, *args):\n",
    "    cp_mat = np.zeros((len(checkpoints), len(checkpoints))).astype(int) - 1\n",
    "    datas, k, kk = position_extension(x, y, ext_width)\n",
    "    print(k, kk, ext_width)\n",
    "    for i in range(cp_mat.shape[0]):\n",
    "        for j in range(cp_mat.shape[1]):\n",
    "            gap = abs(checkpoints[i] - checkpoints[j]) * 2\n",
    "            gap = min(gap, 100)\n",
    "            if i == j:\n",
    "                gap = 80\n",
    "            local_win_widths = np.arange(max(10, gap-20), gap+20, 2)\n",
    "            cp_mat[i][j] = signal_comparison(datas[0], datas[1],\n",
    "                                             local_win_widths, k,\n",
    "                                             checkpoints[i], checkpoints[j], args)\n",
    "    \n",
    "    return cp_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f06df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GT: {changepoints}')\n",
    "\n",
    "xa = []\n",
    "for det_cp in np.where(slice_norm_signal > 0.25)[0]:\n",
    "    xa.append(climb_mountain(slice_norm_signal, det_cp, seuil=5))\n",
    "xa = np.unique(xa)\n",
    "\n",
    "print(xa)\n",
    "print(xa[np.argsort(slice_norm_signal[xa])[::-1]])\n",
    "check_arange = np.arange(50, 100, 2) #WIN_WIDTHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691817c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_mat = check_pts_mat(xa, x, y,\n",
    "                        check_arange, EXT_WIDTH, [56])\n",
    "print(cor_mat)\n",
    "plt.figure(figsize = (11,8))\n",
    "sns.heatmap(cor_mat, xticklabels=xa, yticklabels=xa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(length):\n",
    "    reg_model_num = -1\n",
    "    seuil = -1\n",
    "    if length < 8:\n",
    "        reg_model_num = 5\n",
    "        seuil = 0.60\n",
    "        \n",
    "    elif length < 12:\n",
    "        reg_model_num = 8\n",
    "        seuil = 0.50\n",
    "\n",
    "    elif length < 16:\n",
    "        reg_model_num = 12\n",
    "        seuil = 0.35\n",
    "\n",
    "    elif length < 32:\n",
    "        reg_model_num = 16\n",
    "        seuil = 0.30\n",
    "\n",
    "    elif length < 48:\n",
    "        reg_model_num = 32\n",
    "        seuil = 0.25\n",
    "\n",
    "    elif length < 64:\n",
    "        reg_model_num = 48\n",
    "        seuil = 0.20\n",
    "\n",
    "    elif length < 128:\n",
    "        reg_model_num = 64\n",
    "        seuil = 0.15\n",
    "\n",
    "    elif length < 144:\n",
    "        reg_model_num = 128\n",
    "        seuil = 0.10\n",
    "\n",
    "    else:\n",
    "        reg_model_num = 144\n",
    "        seuil = 0.10\n",
    "\n",
    "    return reg_model_num, seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdb7e9-7a0b-4a5d-b124-9bec1dd5606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvt_2_signal(x, y):\n",
    "    rad_list = radius_list(x, y)\n",
    "    x = x / (np.std(x))\n",
    "    x = np.cumsum(abs(uncumulate(x))) / len(x)\n",
    "    y = y / (np.std(y))\n",
    "    y = np.cumsum(abs(uncumulate(y))) / len(y)\n",
    "    return np.vstack((x, rad_list)).T, np.vstack((y, rad_list)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeb41e-b141-44c0-9d31-0bc637084161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_trajectory(x, y, cps):\n",
    "    if len(cps) == 0:\n",
    "        return [x], [y]\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    for i in range(1, len(cps)):\n",
    "        new_x.append(x[cps[i-1]:cps[i]].copy())\n",
    "        new_y.append(y[cps[i-1]:cps[i]].copy())\n",
    "    return new_x, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1dc80-66e1-44fb-b4e1-be2467391b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_nums = [5, 8, 12, 16, 32, 48, 64, 128, 144]\n",
    "reg_models = {n:tf.keras.models.load_model(f'./models/alpha_reg_models/reg_model_{n}.keras') for n in reg_model_nums}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5951be6-76dd-4e5b-adb6-7113cb73653f",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac858d9-2198-490d-94e2-74a2fe31a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_signal(signal, cps):\n",
    "    sort_indice = np.argsort(signal[cps])\n",
    "    indice_tuple = [(i, i+1) for i in sort_indice]\n",
    "    return indice_tuple, sort_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d1230-25c6-4f89-918d-af7a1c7fbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoupe_trajectory(x, y, model_num, jump=1):\n",
    "    couped_x = []\n",
    "    couped_y = []\n",
    "    for i in range(0, len(x), jump):\n",
    "        tmp1 = x[i: i+model_num]\n",
    "        tmp2 = y[i: i+model_num]\n",
    "        if len(tmp1) == model_num:\n",
    "            couped_x.append(tmp1)\n",
    "            couped_y.append(tmp2)\n",
    "    return np.array(couped_x), np.array(couped_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd44eaf-599c-4eb3-a40c-0e567f8ecbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_cps_search(x, y, win_widths, ext_width, search_seuil=0.20,\n",
    "                          prior_alpha=None, cluster=None, popts=None):\n",
    "    if len(x) < np.min(reg_model_nums):\n",
    "        return np.array([0, len(x)]), np.array([1.0]), np.array([0.1]), np.array([len(x)])\n",
    "    \n",
    "    if cluster is not None and len(cluster.means_) == 1:\n",
    "        start_cps = []\n",
    "        slice_norm_signal = np.zeros_like(x.shape)\n",
    "    else:\n",
    "        if len(x) + 2*(len(x)-1) >= win_widths[0]:\n",
    "            signals, sliced_signals, _, _, denoised_density, l_ext,_ = signal_from_extended_data(x, y,\n",
    "                                                                                        win_widths,\n",
    "                                                                                        ext_width,\n",
    "                                                                                        1,\n",
    "                                                                                        10)\n",
    "            slice_norm_signal = slice_normalize(sliced_signals)\n",
    "\n",
    "            det_cps = []\n",
    "            for det_cp in np.where(slice_norm_signal > search_seuil)[0]:\n",
    "                det_cps.append(climb_mountain(slice_norm_signal, det_cp, seuil=5))\n",
    "            det_cps = np.unique(det_cps)\n",
    "            start_cps = list(det_cps.copy())\n",
    "        else:\n",
    "            start_cps = []\n",
    "            slice_norm_signal = np.zeros_like(x.shape)\n",
    "\n",
    "    start_cps.append(0)\n",
    "    start_cps.append(len(x))\n",
    "    start_cps = np.sort(start_cps)\n",
    "    cps_copy = [0]\n",
    "    for i in range(1, len(start_cps)-1):\n",
    "        if start_cps[i] - start_cps[i-1] > 5 and start_cps[i+1] - start_cps[i] > 5:\n",
    "            cps_copy.append(start_cps[i])\n",
    "    start_cps = cps_copy\n",
    "    start_cps.append(len(x))\n",
    "    #print('First cps: ',start_cps)\n",
    "    \n",
    "    while True:\n",
    "        filtered_cps = []\n",
    "        alpha_preds = []\n",
    "        seuil_preds = []\n",
    "        k_preds = []\n",
    "        \n",
    "        part_xs, part_ys = partition_trajectory(x, y, start_cps)\n",
    "        for p_x, p_y in zip(part_xs, part_ys):\n",
    "            input_signals = []\n",
    "            disp = []\n",
    "            \n",
    "            model_num, seuil = model_selection(len(p_x))\n",
    "            #model_num, seuil = 16, 0.2\n",
    "            model = reg_models[model_num]\n",
    "            #print(p_x.shape, model_num)\n",
    "            re_couped_x, re_couped_y = recoupe_trajectory(p_x, p_y, model_num)\n",
    "            for r_x, r_y in zip(re_couped_x, re_couped_y):\n",
    "                input_signal1, input_signal2 = cvt_2_signal(r_x, r_y)\n",
    "                input_signals.append(input_signal1)\n",
    "                input_signals.append(input_signal2)\n",
    "\n",
    "            input_signals = np.array(input_signals).reshape(-1, model_num, 1, 2)\n",
    "            pred_alpha = model.predict(input_signals, verbose=0).flatten()\n",
    "            #print(pred_alpha)\n",
    "            if len(pred_alpha) > 4:\n",
    "                pred_alpha = np.sort(pred_alpha)[int(0.25 * len(pred_alpha)): int(0.75 * len(pred_alpha))].mean()\n",
    "            else:\n",
    "                pred_alpha = np.mean(pred_alpha)\n",
    "            \n",
    "            for j in range(1, len(p_x)):\n",
    "                x_seg = p_x[j] - p_x[j-1]\n",
    "                y_seg = p_y[j] - p_y[j-1]\n",
    "                disp.append(np.sqrt(x_seg**2 + y_seg**2) / 4.)\n",
    "                \n",
    "            k_preds.append(np.mean(disp))\n",
    "            alpha_preds.append(pred_alpha)\n",
    "            seuil_preds.append(seuil)\n",
    "            \n",
    "        delete_cps = -1\n",
    "        if cluster is not None:\n",
    "            sorted_indice_tuple, sorted_indice = sort_by_signal(slice_norm_signal, start_cps[1:-1])\n",
    "            for (l, r), i in zip(sorted_indice_tuple, sorted_indice):\n",
    "                i += 1\n",
    "                diff_alpha = abs(alpha_preds[l] - alpha_preds[r])\n",
    "                diff_seuil = (seuil_preds[l] + seuil_preds[r])\n",
    "\n",
    "                cluster_pred = cluster.predict([[alpha_preds[l], k_preds[l]], [alpha_preds[r], k_preds[r]]])\n",
    "                #print(alpha_preds)\n",
    "                #print(start_cps, cluster_pred)\n",
    "                if cluster_pred[0] == cluster_pred[1]:\n",
    "                    delete_cps = start_cps[i]\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    print(cluster_pred)\n",
    "                    print('------------------------')\n",
    "                    \n",
    "                    print('entered:', start_cps)\n",
    "                    print(alpha_preds[l], alpha_preds[r])\n",
    "                    print(prior_alpha.pdf(alpha_preds[l]), prior_alpha.pdf(alpha_preds[r]))\n",
    "                    print('------------------------')\n",
    "                    p_mean = np.sum(prior_alpha.pdf(alpha_preds[l]) + prior_alpha.pdf(alpha_preds[r]))/2.\n",
    "                    from_cp = start_cps[i-1]\n",
    "                    if i+1 > len(start_cps)-1:\n",
    "                        to_cp = len(x)\n",
    "                    else:\n",
    "                        to_cp = start_cps[i+1]\n",
    "                    input_signals = []\n",
    "                    model_num, seuil = model_selection(to_cp - from_cp)\n",
    "                    model = reg_models[model_num]\n",
    "\n",
    "                    re_couped_x, re_couped_y = recoupe_trajectory(x[from_cp:to_cp], y[from_cp:to_cp], model_num)\n",
    "                    for r_x, r_y in zip(re_couped_x, re_couped_y):\n",
    "                        input_signal1, input_signal2 = cvt_2_signal(r_x, r_y)\n",
    "                        input_signals.append(input_signal1)\n",
    "                        input_signals.append(input_signal2)\n",
    "\n",
    "                    input_signals = np.array(input_signals).reshape(-1, model_num, 1, 2)\n",
    "                    pred_alpha = model.predict(input_signals, verbose=0).flatten()\n",
    "                    pred_alpha = np.mean(pred_alpha)\n",
    "                    p_merged = prior_alpha.pdf(pred_alpha)\n",
    "                    print('merged_alpha:',pred_alpha, ' --- pmerged:',p_merged, ' ---- pmean:',p_mean)\n",
    "                    if p_merged >= p_mean:\n",
    "                        delete_cps = start_cps[i]\n",
    "                        break\n",
    "                    \"\"\"\n",
    "\n",
    "        if delete_cps == -1:\n",
    "            filtered_cps = start_cps\n",
    "            break\n",
    "        else:\n",
    "            start_cps.remove(delete_cps)\n",
    "            \n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "    if cluster is not None:\n",
    "        adjusted_alphas = []\n",
    "        non_sup_preds = cluster.predict(np.vstack((np.zeros_like(alpha_preds), alpha_preds)).T)\n",
    "        cluster.means_[:,1]\n",
    "        for nsp in non_sup_preds:\n",
    "            adjusted_alphas.append(cluster.means_[:,1][nsp])\n",
    "        alpha_preds = adjusted_alphas\n",
    "    \"\"\"\n",
    "\n",
    "    seg_lengths = uncumulate(np.array(filtered_cps))[1:]\n",
    "    \n",
    "    alpha_preds = np.array(alpha_preds)\n",
    "    filtered_cps = np.array(filtered_cps)\n",
    "    k_preds = np.array(k_preds)\n",
    "    return filtered_cps, alpha_preds, k_preds, seg_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb6e37-dd7a-4b55-a5df-b53135b8f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GT: {changepoints}')\n",
    "detected_points = exhaustive_cps_search(x, y, WIN_WIDTHS, EXT_WIDTH, 0.25)\n",
    "print(detected_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672cda42-e5d8-4cc4-afb8-2c8b1fe5c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x, y, linewidth=0.8)\n",
    "for dtp in detected_points[0][1:-1]:\n",
    "    plt.scatter(x[dtp], y[dtp], marker='+', c='green')\n",
    "plt.xlim([np.min(x) - 15,\n",
    "                  np.min(x) + max(np.max(x) - np.min(x), \n",
    "                    np.max(y) - np.min(y)) + 15])\n",
    "plt.ylim([np.min(y) - 15,\n",
    "                  np.min(y) + max(np.max(x) - np.min(x), \n",
    "                    np.max(y) - np.min(y)) + 15])\n",
    "plt.scatter(x[0], y[0], marker='>', c='red')\n",
    "plt.scatter(x[-1], y[-1], marker='<', c='blue')\n",
    "for cp in changepoints:\n",
    "    plt.scatter(x[cp], y[cp], marker='+', c='blue', zorder=2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(x, y, np.arange(signals.shape[1]), linewidth=0.8, alpha=0.7)\n",
    "ax.set_xlim([np.min(x) - 15,\n",
    "                  np.min(x) + max(np.max(x) - np.min(x), \n",
    "                    np.max(y) - np.min(y)) + 15])\n",
    "ax.set_ylim([np.min(y) - 15,\n",
    "                  np.min(y) + max(np.max(x) - np.min(x), \n",
    "                    np.max(y) - np.min(y)) + 15])\n",
    "ax.scatter3D(x[0], y[0], 0, marker='>', c='red')\n",
    "ax.scatter3D(x[-1], y[-1], signals.shape[1], marker='<', c='blue')\n",
    "\n",
    "for cp in changepoints:\n",
    "    ax.scatter3D(x[cp], y[cp], np.arange(signals.shape[1])[cp], marker='+', c='blue', zorder=2)\n",
    "for dtp in detected_points[0]:\n",
    "    ax.scatter3D(x[dtp], y[dtp], np.arange(signals.shape[1])[dtp], marker='+', c='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0466e2e-2c77-4ec6-a8d7-99dc99ce7462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a197d27-b077-4d5a-bc4d-2c3d3c25f433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3270b1b-cf33-454c-8cde-6d079e5e341c",
   "metadata": {},
   "source": [
    "# Produce results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss2d(x, mu_x, mu_y, sigma_x, sigma_y, norm_factor):\n",
    "    return norm_factor * np.exp(-1/2 * (((x[0] - mu_x)/sigma_x)**2 + ((x[1] - mu_y)/sigma_y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c197e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss1d(x, mu, fixed_sigma, norm_fac):\n",
    "    #fixed_sigma = 1.0\n",
    "    return norm_fac * np.exp((-1/2) * ((x - mu) / fixed_sigma) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb1b55-a5ac-48dd-aff0-24e920ff0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_data_path = f'public_data_validation_v1/' # make sure the folder has this name or change it\n",
    "submit_number = 1\n",
    "path_results = f'result_validation_{submit_number}/'\n",
    "if not os.path.exists(path_results):\n",
    "    os.makedirs(path_results)\n",
    "\n",
    "N_EXP = 13\n",
    "N_FOVS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac1fdd-2a6c-435c-8caf-7940764467b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for track in [1,2]:\n",
    "    \n",
    "    # Create the folder of the track if it does not exists\n",
    "    path_track = path_results + f'track_{track}/'\n",
    "    if not os.path.exists(path_track):\n",
    "        os.makedirs(path_track)\n",
    "        \n",
    "    for exp in range(N_EXP):\n",
    "        # Create the folder of the experiment if it does not exits\n",
    "        path_exp = path_track+f'exp_{exp}/'\n",
    "        if not os.path.exists(path_exp):\n",
    "            os.makedirs(path_exp)\n",
    "        \"\"\"\n",
    "        file_name = path_exp + 'ensemble_labels.txt'\n",
    "        \n",
    "        with open(file_name, 'a') as f:\n",
    "            # Save the model (random) and the number of states (2 in this case)\n",
    "            model_name = np.random.choice(datasets_phenom().avail_models_name, size = 1)[0]\n",
    "            f.write(f'model: {model_name}; num_state: {2} \\n')\n",
    "\n",
    "            # Create some dummy data for 2 states. This means 2 columns\n",
    "            # and 5 rows\n",
    "            data = np.random.rand(5, 2)\n",
    "            \n",
    "            data[-1,:] /= data[-1,:].sum()\n",
    "\n",
    "            # Save the data in the corresponding ensemble file\n",
    "            np.savetxt(f, data, delimiter = ';')\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e218bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of experiments and number of FOVS\n",
    "print(f'Submit number: {submit_number}')\n",
    "for track in [0]:\n",
    "    path_track = path_results + f'track_{track}/'\n",
    "\n",
    "    for exp in range(N_EXP):\n",
    "        path_exp = path_track + f'exp_{exp}/'\n",
    "        print(f'Track: {track}, Exp: {exp}')\n",
    "        try:\n",
    "            loaded = np.load(f'{path_results}/priors_{track}_{exp}.npz')\n",
    "            all_alphas = loaded['alphas']\n",
    "            all_seg_lengths = loaded['seg_lengths']\n",
    "            all_ks = loaded['all_ks']\n",
    "            no_priors = False\n",
    "        except:\n",
    "            no_priors = True\n",
    "\n",
    "        if no_priors:\n",
    "            all_seg_lengths = []\n",
    "            all_alphas = []\n",
    "            all_ks = []\n",
    "            \n",
    "            for fov in range(N_FOVS):\n",
    "                # We read the corresponding csv file from the public data and extract the indices of the trajectories:\n",
    "                if track == 2:\n",
    "                    df = pd.read_csv(public_data_path + f'track_{track}/exp_{exp}/trajs_fov_{fov}.csv')\n",
    "                else:\n",
    "                    df = pd.read_csv(public_data_path + f'track_{track}/exp_{exp}/videos_fov_{fov}_track.csv')\n",
    "                traj_idx = np.sort(df.traj_idx.unique())\n",
    "\n",
    "                # Loop over each index\n",
    "                for idx in traj_idx:\n",
    "                    # Get the lenght of the trajectory\n",
    "                    x = np.array(df[df.traj_idx == idx])[:, 2]\n",
    "                    y = np.array(df[df.traj_idx == idx])[:, 3]\n",
    "                    length_traj = df[df.traj_idx == idx].shape[0]\n",
    "                \n",
    "                    cps, alphas, ks, seg_lengths = exhaustive_cps_search(x, y,\n",
    "                                                                         WIN_WIDTHS,\n",
    "                                                                         EXT_WIDTH,\n",
    "                                                                         search_seuil=0.40)\n",
    "                    all_alphas.extend(alphas)\n",
    "                    all_seg_lengths.extend(seg_lengths)\n",
    "                    all_ks.extend(ks)\n",
    "\n",
    "            all_alphas = np.array(all_alphas)\n",
    "            all_seg_lengths = np.array(all_seg_lengths)\n",
    "            all_ks = np.array(all_ks)\n",
    "            np.savez(f'{path_results}/priors_{track}_{exp}.npz', alphas=all_alphas, seg_lengths=all_seg_lengths, all_ks=all_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573df5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of experiments and number of FOVS\n",
    "print(f'Submit number: {submit_number}')\n",
    "for track in [2]:\n",
    "    path_track = path_results + f'track_{track}/'\n",
    "\n",
    "    for exp in range(N_EXP):\n",
    "        path_exp = path_track + f'exp_{exp}/'\n",
    "        print(f'Track: {track}, Exp: {exp}')\n",
    "        try:\n",
    "            #loaded = np.load(f'{path_results}/old_priors_0.20_seuil/old_priors_{track}_{exp}.npz')\n",
    "            loaded = np.load(f'{path_results}/priors_{track}_{exp}.npz')\n",
    "            all_alphas = loaded['alphas']\n",
    "            all_seg_lengths = loaded['seg_lengths']\n",
    "            all_ks = loaded['all_ks']\n",
    "            no_priors = False\n",
    "        except:\n",
    "            no_priors = True\n",
    "            print(f'No priors for exp:{exp}')\n",
    "            \n",
    "        print('--- post processing ---')\n",
    "        sampling_nb = 1000\n",
    "        alpha_range = np.linspace(-0.2, 2.2, 100)\n",
    "        k_range = np.linspace(-0.2, np.max(all_ks), 100)\n",
    "        all_alphas = all_alphas[np.argwhere((all_seg_lengths > 32) & (all_seg_lengths < 256)).flatten()]\n",
    "        all_ks = all_ks[np.argwhere((all_seg_lengths > 32) & (all_seg_lengths < 256)).flatten()]\n",
    "\n",
    "        H, xedges, yedges = np.histogram2d(all_alphas, all_ks, bins=[alpha_range, k_range])\n",
    "        H = H / np.sum(H)\n",
    "\n",
    "\n",
    "        poten_nb_states = 0\n",
    "        for data, data_range in zip([all_alphas, all_ks], [alpha_range, k_range]):\n",
    "            nb_states = 0\n",
    "            weight_sum = 0\n",
    "            \n",
    "            hist = np.histogram(data, bins=data_range)\n",
    "            prior_info = scipy.stats.rv_histogram(hist, density=True)\n",
    "            p_ = prior_info.pdf(data_range)\n",
    "            p_ = p_ / np.sum(p_)\n",
    "            samples = np.random.choice(data_range, p=p_, size=5000)\n",
    "            samples = np.vstack((np.zeros_like(samples), samples)).T\n",
    "            bgm = BayesianGaussianMixture(n_components=5, max_iter=2000, n_init=10).fit(samples)\n",
    "            \n",
    "            for mean, weight in zip(bgm.means_[np.argsort(bgm.weights_)[::-1]],\n",
    "                                    bgm.weights_[np.argsort(bgm.weights_)[::-1]]):\n",
    "                weight_sum += weight\n",
    "                nb_states += 1\n",
    "                if weight_sum >= 0.90:\n",
    "                    break\n",
    "            poten_nb_states = max(poten_nb_states, nb_states)\n",
    "            print(bgm.weights_)\n",
    "            print(f'nb_state:{nb_states}')\n",
    "            \n",
    "        print(f'Estimated nb clusters: {poten_nb_states}')\n",
    "        cluster = GaussianMixture(n_components=poten_nb_states, max_iter=1000, n_init=10,\n",
    "                                          covariance_type='diag').fit(np.vstack((all_alphas, all_ks)).T)\n",
    "        print('Cluster centers: ', cluster.means_)\n",
    "        print(cluster.weights_)\n",
    "        print(cluster.n_features_in_)\n",
    "\n",
    "        for fov in range(N_FOVS):\n",
    "            # We read the corresponding csv file from the public data and extract the indices of the trajectories:\n",
    "            if track == 2:\n",
    "                df = pd.read_csv(public_data_path + f'track_{track}/exp_{exp}/trajs_fov_{fov}.csv')\n",
    "            else:\n",
    "                df = pd.read_csv(public_data_path + f'track_{track}/exp_{exp}/videos_fov_{fov}_track.csv')\n",
    "            traj_idx = np.sort(df.traj_idx.unique())\n",
    "            submission_file = path_exp + f'fov_{fov}.txt'\n",
    "            with open(submission_file, 'w') as f:\n",
    "                # Loop over each index\n",
    "                for idx in traj_idx:\n",
    "                    # Get the lenght of the trajectory\n",
    "                    x = np.array(df[df.traj_idx == idx])[:, 2]\n",
    "                    y = np.array(df[df.traj_idx == idx])[:, 3]\n",
    "                    length_traj = df[df.traj_idx == idx].shape[0]\n",
    "\n",
    "                    cps, alphas, ks, _ = exhaustive_cps_search(x, y, WIN_WIDTHS,\n",
    "                                                               EXT_WIDTH,\n",
    "                                                               search_seuil=0.25,\n",
    "                                                               cluster=cluster,\n",
    "                                                               popts=popts)\n",
    "\n",
    "                    prediction_traj = [idx.astype(int)]\n",
    "                    for k, alpha, state, cp in zip(ks, alphas, [99999999] * len(cps), cps[1:]):\n",
    "                        prediction_traj.append(k)\n",
    "                        prediction_traj.append(alpha)\n",
    "                        prediction_traj.append(state)\n",
    "                        prediction_traj.append(cp)\n",
    "\n",
    "                    formatted_numbers = ','.join(map(str, prediction_traj))\n",
    "                    f.write(formatted_numbers + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58e06c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('alpha')\n",
    "plt.hist(all_alphas, bins=alpha_range)\n",
    "plt.figure()\n",
    "plt.title('K')\n",
    "plt.hist(all_ks, bins=k_range)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('alpha, k')\n",
    "plt.hist2d(all_alphas, all_ks, bins=[xedges, yedges])\n",
    "plt.figure()\n",
    "plt.hist2d(all_alphas, all_ks, bins=[xedges, np.linspace(0.0, 0.2, 25)])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Mixture boundary\")\n",
    "ZX, ZY = np.meshgrid(alpha_range, k_range)\n",
    "Z = np.stack((ZX, ZY)).T.reshape(-1, 2)\n",
    "ZC = cluster.predict(Z)\n",
    "plt.scatter(Z[:,0], Z[:,1], c=ZC, s=0.7, alpha=0.3, zorder=1)\n",
    "plt.imshow(H.T, extent=(alpha_range[0], alpha_range[-1], k_range[0], k_range[-1]), origin='lower', alpha=1.0)\n",
    "#plt.hist2d(all_alphas, all_ks, bins=[xedges, yedges])\n",
    "for cluster_mean in cluster.means_:\n",
    "    plt.scatter(cluster_mean[0], cluster_mean[1], marker='+', c='red')\n",
    "    print(cluster.predict([[cluster_mean[0], cluster_mean[1]]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e36702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c92fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ee463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad0db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e0e12c-0a63-4ded-bf8b-7ecfdaa1d7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the number of experiments and number of FOVS\n",
    "print(f'Submit number: {submit_number}')\n",
    "for track in [2]:\n",
    "    path_track = path_results + f'track_{track}/'\n",
    "\n",
    "    for exp in [0]:\n",
    "        path_exp = path_track + f'exp_{exp}/'\n",
    "        print(f'Track: {track}, Exp: {exp}')\n",
    "        try:\n",
    "            #loaded = np.load(f'{path_results}/old_priors_0.20_seuil/old_priors_{track}_{exp}.npz')\n",
    "            loaded = np.load(f'{path_results}/priors_{track}_{exp}.npz')\n",
    "            all_alphas = loaded['alphas']\n",
    "            all_seg_lengths = loaded['seg_lengths']\n",
    "            all_ks = loaded['all_ks']\n",
    "            no_priors = False\n",
    "        except:\n",
    "            no_priors = True\n",
    "            print(f'No priors for exp:{exp}')\n",
    "            \n",
    "        print('--- post processing ---')\n",
    "        sampling_nb = 1000\n",
    "        alpha_range = np.linspace(-0.2, 2.2, 100)\n",
    "        k_range = np.linspace(-0.2, np.max(all_ks), 100)\n",
    "        all_alphas = all_alphas[np.argwhere((all_seg_lengths > 32) & (all_seg_lengths < 256)).flatten()]\n",
    "        all_ks = all_ks[np.argwhere((all_seg_lengths > 32) & (all_seg_lengths < 256)).flatten()]\n",
    "        print(all_alphas.shape)\n",
    "        print(all_ks.shape)\n",
    "        print(np.argwhere(all_seg_lengths > 64).shape)\n",
    "\n",
    "        H, xedges, yedges = np.histogram2d(all_alphas, all_ks, bins=[alpha_range, k_range])\n",
    "        H = H / np.sum(H)\n",
    "        poten_nb_states = 1\n",
    "        for data, data_range in zip([all_alphas, all_ks], [alpha_range, k_range]):\n",
    "            hist = np.histogram(data, bins=data_range)\n",
    "            prior_info = scipy.stats.rv_histogram(hist, density=True)\n",
    "            p_ = prior_info.pdf(data_range)\n",
    "            p_ = p_ / np.sum(p_)\n",
    "            samples = np.random.choice(data_range, p=p_, size=3000)\n",
    "            samples = np.vstack((np.zeros_like(samples), samples)).T\n",
    "            bgm = BayesianGaussianMixture(n_components=4, max_iter=1000).fit(samples)\n",
    "            popts = []\n",
    "            pcovs = []\n",
    "            for bgm_mean in bgm.means_[:, 1]:\n",
    "                popt, pcov = scipy.optimize.curve_fit(gauss1d, data_range,\n",
    "                                                      prior_info.pdf(data_range),\n",
    "                                                      p0=[bgm_mean, 0.1, 1])\n",
    "                if len(popts) == 0:\n",
    "                    popts.append(popt)\n",
    "                    pcovs.append(pcov)\n",
    "                else:\n",
    "                    flag = 1\n",
    "                    for comp_popt in popts:\n",
    "                        if not np.allclose(comp_popt, popt, rtol=1e-1):\n",
    "                            flag *= 1\n",
    "                        else:\n",
    "                            flag *= 0\n",
    "                    if flag == 1:\n",
    "                        popts.append(popt)\n",
    "                        pcovs.append(pcov)\n",
    "            popts = np.array(popts)\n",
    "            pcovs = np.array(pcovs)\n",
    "            poten_nb_states = max(poten_nb_states, len(popts))\n",
    "            print(len(popts))\n",
    "            print(popts)\n",
    "            print(bgm.n_features_in_)\n",
    "            print(bgm.means_)\n",
    "            print(bgm.weights_)\n",
    "            \n",
    "        print('Number of clusters: ', poten_nb_states)\n",
    "        cluster = GaussianMixture(n_components=poten_nb_states, max_iter=10, n_init=10,\n",
    "                                  covariance_type='diag').fit(np.vstack((all_alphas, all_ks)).T)\n",
    "        \n",
    "        print('Cluster centers: ', cluster.means_)\n",
    "\n",
    "        for fov in range(N_FOVS):\n",
    "            # We read the corresponding csv file from the public data and extract the indices of the trajectories:\n",
    "            if track == 2:\n",
    "                df = pd.read_csv(public_data_path + f'track_{track}/exp_{exp}/trajs_fov_{fov}.csv')\n",
    "            else:\n",
    "                df = pd.read_csv(public_data_path + f'track_{track}/exp_{exp}/videos_fov_{fov}_track.csv')\n",
    "            traj_idx = np.sort(df.traj_idx.unique())\n",
    "            submission_file = path_exp + f'fov_{fov}.txt'\n",
    "            with open(submission_file, 'w') as f:\n",
    "                # Loop over each index\n",
    "                for idx in traj_idx:\n",
    "                    # Get the lenght of the trajectory\n",
    "                    x = np.array(df[df.traj_idx == idx])[:, 2]\n",
    "                    y = np.array(df[df.traj_idx == idx])[:, 3]\n",
    "                    length_traj = df[df.traj_idx == idx].shape[0]\n",
    "\n",
    "                    cps, alphas, ks, _ = exhaustive_cps_search(x, y, WIN_WIDTHS,\n",
    "                                                               EXT_WIDTH,\n",
    "                                                               search_seuil=0.25,\n",
    "                                                               cluster=cluster,\n",
    "                                                               popts=popts)\n",
    "\n",
    "                    prediction_traj = [idx.astype(int)]\n",
    "                    for k, alpha, state, cp in zip(ks, alphas, [99999999] * len(cps), cps[1:]):\n",
    "                        prediction_traj.append(k)\n",
    "                        prediction_traj.append(alpha)\n",
    "                        prediction_traj.append(state)\n",
    "                        prediction_traj.append(cp)\n",
    "\n",
    "                    formatted_numbers = ','.join(map(str, prediction_traj))\n",
    "                    f.write(formatted_numbers + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ff749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random independent variable\n",
    "x = np.random.randn(300)\n",
    "\n",
    "# Generate random noise for each dependent variable\n",
    "noise = np.random.randn(300)\n",
    "\n",
    "# Generate dependent variables based on the independent variable and noise\n",
    "y = 2 * x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47896669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_x_given_y(x, y):\n",
    "    # Specify the parameters of the conditional distribution\n",
    "    # You can use any appropriate distribution based on your problem\n",
    "    mean_x = np.mean(x)\n",
    "    var_x = np.var(x)\n",
    "    print(var_x)\n",
    "    precision_x = 1 / var_x\n",
    "    \n",
    "    # Sample from the conditional distribution\n",
    "    sampled_x = np.random.normal((np.mean(y) * precision_x) / (len(y) * precision_x + 1), np.sqrt(1 / (len(y) * precision_x + 1)))\n",
    "    \n",
    "    return sampled_x\n",
    "\n",
    "# Conditional distribution of each dependent variable given the independent variable\n",
    "def p_y_given_x(x, y):\n",
    "    # Specify the parameters of the conditional distribution\n",
    "    # You can use any appropriate distribution based on your problem\n",
    "    mean_y = np.mean(y)\n",
    "    var_y = np.var(y)\n",
    "    print(var_y)\n",
    "    precision_y = 1 / var_y\n",
    "    \n",
    "    # Sample from the conditional distribution for each dependent variable\n",
    "    sampled_y = np.random.normal(mean_y + precision_y * (y - 2 * x), np.sqrt(1 / (len(y) * precision_y + 1)))\n",
    "    \n",
    "    return sampled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd12966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "num_iterations = 1000\n",
    "samples_x = np.zeros(num_iterations)\n",
    "samples_y = np.zeros((num_iterations, len(y)))\n",
    "\n",
    "# Initialize starting values for x and y\n",
    "current_x = np.random.randn()\n",
    "current_y = np.random.randn(len(y))\n",
    "\n",
    "# Perform Gibbs sampling\n",
    "for i in range(num_iterations):\n",
    "    # Sample x given y\n",
    "    current_x = p_x_given_y(current_x, current_y)\n",
    "    samples_x[i] = current_x\n",
    "    \n",
    "    # Sample y given x\n",
    "    current_y = p_y_given_x(current_x, current_y)\n",
    "    samples_y[i, :] = current_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean and standard deviation of the samples\n",
    "print(\"Mean of x: \", np.mean(samples_x))\n",
    "print(\"Standard deviation of x: \", np.std(samples_x))\n",
    "print(\"Mean of y: \", np.mean(samples_y, axis=0))\n",
    "print(\"Standard deviation of y: \", np.std(samples_y, axis=0))\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(samples_x)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('x')\n",
    "plt.title('Gibbs Sampling - Independent Variable')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(len(y)):\n",
    "    plt.plot(samples_y[:, i])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gibbs Sampling - Dependent Variables')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22132788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
